ls
3
clear
data
3+5
3+5
3+5
3+5
load('results.RData')
p<-results[,1]
R2<-results[,2]
R2.adj<-results[,3]
PRESS<-results[,4]
AIC<-results[,5]
BIC<-results[,6]
Cp<-results[,7]
load('results.RData')
print(mse)
library(foreach)
library(doMC)
interactive()
library(foreach)
library(doMC)
registerDoMC(2)  #change the 2 to your number of CPU cores
foreach(i=1:10) %dopar% {
print(i)
}
library(foreach)
library(doMC)
x = 3
print('x', 3)
cat('x', 3)
lams_orig = 10^(-6:0)
lams_small = .025*(4:39)
lams1 = sort(c(lams_orig,lams_small))
lams1
print(t(lams1))
print((lams1))
print((lams1))
print((lams1))
print((lams1))
print((lams1))
10^-5
10^-4
10^-6
lams_mid = 10^-5 * 2 * (1:4)
f <- func(x){}
f <- function(){
return 5
}
fo <- function(){
return 5
}
fo <- function(){
return 5
}
fo <- function(){
return(5)
}
fo <- function(){
return(5)
}
x <- function(x){
print(x)
}
x(fo)
fo <- function(){
return(5)
}
x <- function(x){
print(x())
}
x(fo)
x = 5
x[[1]]
x[1]
x
pres = c("")
for(pre in pres){
print(pre)
}
graph = matrix(0,161,161)
diag(graph)=1
sum(graph)
which(diff_graph)
which(graph)
graph = matrix(F,161,161)
diag(graph)=T
which(graph)
idxs_list = list(which(graph))
sum(x)
x~y
x = 1:10
y = 5:15
x~y
lm(x~y)
x = 1:10
y = 1:10
x~y
lm(x~y)
dir = '/if19/cs3hq/qilab/'
#dir = '/Users/chandansingh/drive/asdf/research/qi_lab/SinghU-shared/ModelCode/'
data_dir = "/Users/chandansingh/drive/asdf/research/qi_lab/AbideData/ABIDE_ROI_Data-processed/ccs-filt_global/rois_dos2/"
source(paste0(dir,"run_simule/src/funcs.R"))
setwd(paste0(dir,"run_simule/data"))
############# initial load #############
#dir = '/if19/cs3hq/qilab/'
dir = '/Users/chandansingh/drive/asdf/research/singh_connectome/'
data_dir = paste0(dir,"data/abide_data/ABIDE_ROI_Data-processed/ccs-filt_global/rois_dos2/")
source(paste0(dir,"src/main/init.R"))
source('~/drive/asdf/research/singh_connectome/src/main/init.R', echo=TRUE)
source('~/drive/asdf/research/singh_connectome/src/main/init.R', echo=TRUE)
source('~/drive/asdf/research/singh_connectome/src/main/init.R', echo=TRUE)
source(paste0(dir,"src/main/init.R"))
files <- (Sys.glob(paste0("/Users/chandansingh/nilearn_data/ABIDE_pcp/dparsf/nofilt_noglobal","Control/*.1D"))) # Load all of the 'Control' subject samples into one matrix
listOfFiles <- lapply(files, function(x) read.delim(x, header = TRUE, sep = "\t"))
control <- lapply(listOfFiles, function(x) as.matrix(x))
control
files <- (Sys.glob(paste0("/Users/chandansingh/nilearn_data/ABIDE_pcp/dparsf/nofilt_noglobal/*.1D"))) # Load all of the 'Control' subject samples into one matrix
listOfFiles <- lapply(files, function(x) read.delim(x, header = TRUE, sep = "\t"))
control <- lapply(listOfFiles, function(x) as.matrix(x))
source('~/drive/asdf/research/singh_connectome/src/main/main.R', echo=TRUE)
source('~/drive/asdf/research/singh_connectome/src/postprocess/calc_connectome_stats.R', echo=TRUE)
source('~/drive/asdf/research/singh_connectome/src/main/main.R', echo=TRUE)
task_num = 1
args = commandArgs(trailingOnly=TRUE)
if(length(args)>0){
task_num = strtoi(args)
}
cat('task_num ',task_num,'\n')
#dir = '/if19/cs3hq/qilab/'
dir = '/Users/chandansingh/drive/asdf/research/singh_connectome/'
data_dir = paste0(dir,"data/abide_data/cpac_nofilt_noglobal/")
source(paste0(dir,"src/main/init.R"))
View(calc_idxs)
View(calc_idxs)
View(calc_idxs)
d_flat = flatten_classes(data_ll,1,1)
data_con = d_flat[[1]][1:11,1:10]
data_aut = d_flat[[2]][1:12,1:10]
task_num = 1
args = commandArgs(trailingOnly=TRUE)
if(length(args)>0){
task_num = strtoi(args)
}
cat('task_num ',task_num,'\n')
#dir = '/if19/cs3hq/qilab/'
dir = '/Users/chandansingh/drive/asdf/research/singh_connectome/'
data_dir = paste0(dir,"data/abide_data/cpac_nofilt_noglobal/")
source(paste0(dir,"src/main/init.R"))
setwd(paste0(dir,"data"))
# set params
dataset = "full" #values: full,nyu - picks what dataset to use
norm = F #not currently being used, has to do with normalizing orig data
seed=1 #seed for how to split training data
calc_graphs=F
calc_idxs=T
calc_acc=T
calc_conn=T
calc_ll=T
# loading all data
dists = as.matrix(read.table(file="dists.csv",sep=",",header=F, check.names=FALSE)) # dists (160x160)
dists_norm = dists/max(dists)
labels = as.matrix(read.table(file="labels.csv",sep=",",header=F, check.names=FALSE)) # labels (160x1, 40 unique)
networks = as.matrix(read.table(file="networks.csv",sep=",",header=F, check.names=FALSE)) # networks (160x1, 6 unique)
load_ll_data(norm) # load ll data (set cors_nsimule_ll,...,data_ll)
data = load_data(dataset) # load data
format_data(dataset) # nyu, full
partition_data(seed)
# need to calc cors_train
data_train = data[train_idxs]
n_control_train = sum(train_idxs<=N_autism)
data_train_flattened = flatten_classes(data_train,n_control_train,n_autism_train)
n_autism_train = length(train_idxs) - n_control_train
#calc_all_cors(paste0("cors_",dataset,"_",seed),data_train_flattened)
load_dpm()
load_dpm <- function(){
setwd("/Users/chandansingh/drive/asdf/research/singh_connectome/direct_learning/zhao_2014")
dyn.load("dpm.so");
source("dpm.R");
}
# calculate difference in precision matrices
load_dpm()
d_flat = flatten_classes(data_ll,1,1)
data_con = d_flat[[1]][1:11,1:10]
data_aut = d_flat[[2]][1:12,1:10]
fit.aic <- dpm(data_con,data_aut,nlambda=2,tuning="aic")
results = fit.aic
results$dpm
results$lambda
results$lambda[1]
